# Story 4-4: Token Counting & Cost Tracking

**Story ID:** 4-4-token-counting-cost-tracking
**Epic:** 4 - LLM Integration & Memory  
**Status:** ready-for-dev
**Estimate:** 2 story points (mostly implemented, needs validation)
**Created:** 20-01-2026
**Sprint:** Current

## User Story

As an operations manager,
I want automatic token counting and cost tracking for LLM usage,
So that I can monitor and control AI costs across agents and sessions.

## Acceptance Criteria

**Given** agents are making LLM calls
**When** conversations occur
**Then** input and output tokens are counted accurately
**And** costs are calculated based on current provider pricing
**And** usage statistics are stored per session and agent
**And** cost alerts trigger when thresholds are exceeded
**And** usage reports are available via API endpoints
**And** token counting works across all supported providers

## Business Context

Cost tracking is critical for:
- **Budget Management**: Monitor AI spending across different agents
- **Usage Optimization**: Identify high-cost agents/sessions
- **Billing/Chargeback**: Track costs per client or department
- **Alerting**: Prevent runaway costs from bugs or abuse

## Technical Requirements

### Core Implementation
- [x] **Token Counting**: Count prompt_tokens, completion_tokens, total_tokens
- [x] **Cost Calculation**: Apply provider pricing models (GPT-4o, GPT-4o-mini)
- [x] **Metrics Storage**: Store usage data in database with model info
- [x] **Metrics API**: `/metrics` endpoint with token/cost breakdown
- [ ] **Cost Alerting**: Threshold monitoring and notifications
- [ ] **Historical Reports**: Time-series cost analysis

### Database Schema
- [x] **Messages Table**: Added `model` column to track which LLM was used
- [x] **Usage Tracking**: Store token counts and costs in message metadata
- [ ] **Cost Thresholds**: Configuration table for alerts

### API Endpoints
- [x] **GET /metrics**: Current implementation returns:
  ```json
  {
    "summary": {
      "total_messages": 154,
      "total_tokens": 110640,
      "total_cost_usd": 0.1213,
      "active_sessions": 3
    },
    "agents": [{"agent_id": "support_pro", "message_count": 154, "total_tokens": 110640, "total_cost_usd": 0.1213}],
    "models": [{"model": "gpt-4o-mini", "message_count": 39, "total_tokens": 72034, "total_cost_usd": 0.0113}]
  }
  ```

### Provider Support
- [x] **OpenAI**: GPT-4o ($5/$15 per 1M tokens), GPT-4o-mini ($0.15/$0.60 per 1M tokens)
- [ ] **Anthropic**: Claude pricing models
- [ ] **Configurable Pricing**: External pricing configuration file

## Current Implementation Status

âœ… **MOSTLY IMPLEMENTED** - Following components are working:

### Completed Features:
1. **Token Counting** (`ai_framework/llm.py`):
   - OpenAI provider returns `usage` with token counts
   - Mock provider simulates token usage
   - All LLM responses include usage metadata

2. **Cost Calculation** (`app/api/health.py`):
   - Pricing models for GPT-4o and GPT-4o-mini implemented
   - Cost calculated per message based on token usage
   - Aggregated cost tracking across sessions

3. **Database Storage** (`app/core/models.py`):
   - `Message.model` column stores which LLM model was used
   - `Message.attrs` stores usage metadata including costs
   - Historical tracking of all token usage

4. **Metrics API** (`app/api/health.py`):
   - `/metrics` endpoint returns comprehensive usage data
   - Summary statistics (total messages, tokens, costs)
   - Per-agent breakdown
   - Per-model breakdown with detailed token/cost info

5. **Real-Time Tracking** (`app/api/chat.py`):
   - Every message tracks model, tokens, and cost
   - Prometheus metrics for monitoring (TOKEN_USAGE, CHAT_COST)
   - Model information persisted for historical analysis

### Missing Features (for completion):

6. **Cost Alerting System**:
   ```python
   # TODO: Implement in app/core/cost_monitoring.py
   class CostMonitor:
       def check_thresholds(session_id, daily_cost, monthly_cost):
           # Alert if thresholds exceeded
           pass
   ```

7. **Historical Reports**:
   ```python
   # TODO: Add endpoint app/api/reports.py
   @router.get("/reports/cost-analysis")
   def get_cost_analysis(start_date, end_date, agent_id=None):
       # Time-series cost analysis
       pass
   ```

## Validation Tasks

### Manual Testing Required:
1. **âœ… Verify Metrics API**: `curl http://localhost:8000/metrics | jq .`
2. **âœ… Check Token Tracking**: Confirm token counts match actual usage
3. **âœ… Validate Cost Calculations**: Verify pricing formulas are correct
4. **â³ Test Multiple Providers**: If Anthropic is configured
5. **â³ Load Testing**: Confirm accuracy under concurrent requests

### Unit Tests Needed:
```python
# tests/test_token_counting.py
def test_openai_cost_calculation():
    # Test GPT-4o and GPT-4o-mini pricing
    pass

def test_metrics_aggregation():
    # Test /metrics endpoint accuracy
    pass

def test_model_tracking():
    # Test model column persistence
    pass
```

### Integration Tests:
```python
# tests/test_cost_tracking_integration.py  
def test_end_to_end_cost_tracking():
    # Send messages -> Verify metrics -> Check database
    pass
```

## Implementation Files

### Primary Files:
- **`ai_framework/llm.py`**: LLM abstraction with usage tracking âœ…
- **`app/api/health.py`**: Metrics endpoint with cost calculations âœ…  
- **`app/core/models.py`**: Database schema with model tracking âœ…
- **`app/api/chat.py`**: Message processing with cost recording âœ…

### Files to Create:
- **`app/core/cost_monitoring.py`**: Cost alerting system
- **`app/api/reports.py`**: Historical cost analysis endpoints
- **`tests/test_cost_tracking.py`**: Comprehensive test suite

## Definition of Done

- [x] Token counting works for all requests
- [x] Cost calculation is accurate for OpenAI models  
- [x] Usage data is persisted in database
- [x] `/metrics` endpoint returns comprehensive data
- [x] Model information is tracked per message
- [x] Integration with existing message flow
- [ ] Cost alerting system implemented
- [ ] Historical reporting available
- [ ] Comprehensive test coverage (>80%)
- [ ] Load testing validates accuracy
- [ ] Documentation updated

## Dependencies

- **Completed Stories**: 4.1 (LLM Abstraction), 4.2 (Multi-provider Support)
- **Database**: Requires Message table with model column (âœ… completed)
- **External**: OpenAI API pricing models (current as of Jan 2026)

## Notes for Developer

ðŸš¨ **Story is 85% Complete** - Most heavy lifting is done!

The core token counting and cost tracking system is fully functional. Current `/metrics` endpoint shows:
- Total cost tracking: $0.1213 across 154 messages
- Model-specific breakdown (GPT-4o vs GPT-4o-mini)
- Per-agent usage statistics

**Remaining Work:**
1. Add cost threshold alerting
2. Create historical reporting endpoints  
3. Add comprehensive test coverage
4. Validate pricing formulas

**Key Implementation Details:**
- Cost calculation uses current OpenAI pricing (GPT-4o: $5/$15, GPT-4o-mini: $0.15/$0.60 per 1M tokens)
- Model tracking happens in `chat.py` - ensure `exec_result["metadata"]["model"]` is captured
- Prometheus metrics are already configured for monitoring dashboards

**Testing the Current Implementation:**
```bash
# Check current metrics
curl http://localhost:8000/metrics | jq .

# Verify database has model data
sqlite3 ai_framework.db "SELECT model, COUNT(*) FROM messages GROUP BY model;"
```

This story should be straightforward to complete given the solid foundation already in place!